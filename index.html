<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>BaichuanSEED</title>
  <link rel="icon" type="image/x-icon" href="static/images/seeding.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">BaichuanSEED: <u>S</u>haring the Potential of <u>E</u>xtensiv<u>E</u> <u>D</u>ata Collection and Deduplication by Introducing a Competitive Large Language Model Baseline</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Guosheng Dong<sup>1*</sup>,</span>
                <span class="author-block">
                  Da Pan<sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://emanual20.github.io" target="_blank">Yiding Sun</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    Shusen Zhang<sup>1</sup>,</span>
                    <span class="author-block">
                      Zheng Liang<sup>1</sup>,</span>
                      <span class="author-block">
                        Xin Wu<sup>1</sup>,</span>
                        <span class="author-block">
                          Yanjun Shen<sup>1</sup>,</span>
                          <span class="author-block">
                            Fan Yang<sup>1</sup>,</span>
                            <span class="author-block">
                              Haoze Sun<sup>1</sup>,</span>
                              <span class="author-block">
                                Tianpeng Li<sup>1</sup>,</span>
                                <span class="author-block">
                                  Mingan Lin<sup>1</sup>,</span>
                                  <span class="author-block">
                                    Jianhua Xu<sup>1</sup>,</span>
                                    <span class="author-block">
                                      Yufan Zhang<sup>1</sup>,</span>
                                      <span class="author-block">
                                        Xiaonan Nie<sup>1</sup>,</span>
                                        <span class="author-block">
                                          Lei Su<sup>1</sup>,</span>
                                          <span class="author-block">
                                            Bingning Wang<sup>1</sup>,</span>
                                            <span class="author-block">
                                              Wentao Zhang<sup>3</sup>,</span>
                                              <span class="author-block">
                                                Jiaxin Mao<sup>2</sup>,</span>
                                                  <span class="author-block">
                                                    Zenan Zhou<sup>1*</sup>,</span>
                                                    <span class="author-block">
                                                      Weipeng Chen<sup>1</sup></span>                                 
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Baichuan Inc.<sup>1</sup><br> Gaoling School of Artificial Intelligence, Renmin University of China<sup>2</sup><br> Peking University<sup>3</sup></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Authors, {<a href="mailto:dongguosheng@baichuan-inc.com">dongguosheng</a>, <a href="mailto:zhouzenan@baichuan-inc.com">zhouzenan</a>} @baichuan-inc.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(TBU)</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BaichuanSEED/BaichuanSEED.github.io" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Github</span>
                  </a>
                </span>

                  <!-- Huggingface link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/models/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      &#129303;
                    </span>
                    <span>HuggingFace(TBU)</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The general capabilities of Large Language Models (LLM) highly rely on the composition and selection on extensive pretraining datasets, treated as commercial secrets by several institutions. To mitigate this issue, we open-source the details of a universally applicable data processing pipeline and validate its effectiveness and potential by introducing a competitive LLM baseline. Specifically, the data processing pipeline consists of broad collection to scale up and reweighting to improve quality. We then pretrain a 7B model BaichuanSEED with 3T tokens processed by our pipeline without any deliberate downstream task-related optimization, followed by an easy but effective supervised fine-tuning stage. The model demonstrates consistency and predictability throughout training. BaichuanSEED achieves comparable performance on comprehensive benchmarks with several commercial advanced large language models, such as Qwen1.5 and Llama3. We also conduct several heuristic experiments to discuss the potential for further optimization on downstream tasks, such as mathematics and coding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title">Universal Data Processing Pipeline</h2>
        <div class="content has-text-justified">
        <ul>
          <li>Broad Collection: broad collection from trusted sources, mainly including web pages, high knowledge density data, code.</li>
          <li>Reweighting: deduplication and mixture (The details can be found in our technical report)
            <ul>
              <li> Deduplication
              <ul>
                <li>Document-level deduplication globally</li>
                <li>Sentence-level deduplication across documents</li>
                <li>PII and harmful content filtering</li>
              </ul>
              <li> Mixture
                <ul>
                  <li>Heauristic mixture experiments</li>
                </ul>
            </ul>
          </li>
        </ul>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Performance -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance</h2>

        <div class="content has-text-justified">
          <p>
            BaichuanSEED achieves comparable performance with cutting-edge commercial LLMs (Qwen1.5-7B and Llama3-8B), and better performance over existing fully transparent LLMs (OLMO-7B and MAP-Neo-7B). 
          </p>
        </div>

        <h3 class="content">Comprehensive Benchmarks</h3>

        <table style="width:100%; border-collapse:collapse;">
          <thead>
            <tr>
              <th style="width: 26%"></th>
              <th>Training Tokens</th>
              <th>MMLU (5-shot)</th>
              <th>CMMLU (5-shot)</th>
              <th>AGIEval (0-shot)</th>
              <th>C-Eval (5-shot)</th>
              <th>MMLU-Pro (5-shot)</th>
              <th>LiveBench (0-shot)</th>
            </tr>
          </thead>
          <tr>
            <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
          </tr>
          <tbody>
            <tr>
              <td>Baichuan2-7B</td>
              <td>2.6T</td>
              <td>54.65</td>
              <td>56.95</td>
              <td>28.95</td>
              <td>56.19</td>
              <td>21.65</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Baichuan2-13B</td>
              <td>2.6T</td>
              <td>59.83</td>
              <td>61.32</td>
              <td>24.07</td>
              <td>58.10</td>
              <td>26.59</td>
              <td>-</td>
            </tr>
            <tr>
              <td>Qwen1.5-7B</td>
              <td>3T</td>
              <td><u>62.19</u></td>
              <td><b>71.84</b></td>
              <td><b>39.46</b></td>
              <td><b>73.64</b></td>
              <td><u>30.30</u></td>
              <td>-</td>
            </tr>
            <tr>
              <td>Llama3-8B</td>
              <td>15T</td>
              <td><b>66.57</b></td>
              <td>50.68</td>
              <td>26.74</td>
              <td>49.89</td>
              <td><b>35.30</b></td>
              <td>-</td>
            </tr>
            <tr>
              <td>OLMo-7B</td>
              <td>2.5T</td>
              <td>28.40</td>
              <td>25.55</td>
              <td>19.89</td>
              <td>27.27</td>
              <td>13.05</td>
              <td>-</td>
            </tr>
            <tr>
              <td>MAP-Neo-7B</td>
              <td>4.5T</td>
              <td>58.18</td>
              <td>55.06</td>
              <td><u>33.87</u></td>
              <td>57.50</td>
              <td>26.89</td>
              <td>-</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td><b>BaichuanSEED</b></td>
              <td>3T</td>
              <td>60.25</td>
              <td><u>62.09</u></td>
              <td>31.07</td>
              <td><u>61.58</u></td>
              <td>26.57</td>
              <td>-</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td>Baichuan2-7B-Chat</td>
              <td>2.6T</td>
              <td>54.35</td>
              <td>55.36</td>
              <td>35.29</td>
              <td>55.09</td>
              <td>25.11</td>
              <td>12.89</td>
            </tr>
            <tr>
              <td>Baichuan2-13B-Chat</td>
              <td>2.6T</td>
              <td>57.28</td>
              <td><u>61.32</u></td>
              <td>30.15</td>
              <td>58.04</td>
              <td>28.03</td>
              <td>13.04</td>
            </tr>
            <tr>
              <td>Qwen1.5-7B-Chat</td>
              <td>3T</td>
              <td><u>61.49</u></td>
              <td><b>68.02</b></td>
              <td><b>39.29</b></td>
              <td><b>68.96</b></td>
              <td>16.29</td>
              <td>16.78</td>
            </tr>
            <tr>
              <td>Llama3-8B-Instruct</td>
              <td>15T</td>
              <td><b>67.10</b></td>
              <td>51.66</td>
              <td><u>38.37</u></td>
              <td>50.71</td>
              <td><b>41.88</b></td>
              <td><b>25.91</b></td>
            </tr>
            <tr>
              <td>OLMo-7B-SFT</td>
              <td>2.5T</td>
              <td>47.49</td>
              <td>35.49</td>
              <td>29.12</td>
              <td>35.43</td>
              <td>17.99</td>
              <td>8.80</td>
            </tr>
            <tr>
              <td>MAP-Neo-7B-SFT</td>
              <td>4.5T</td>
              <td>58.31</td>
              <td>55.24</td>
              <td>37.98</td>
              <td>55.58</td>
              <td><u>30.24</u></td>
              <td>14.35</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td><b>BaichuanSEED-SFT</b></td>
              <td>3T</td>
              <td>60.15</td>
              <td>60.84</td>
              <td>32.62</td>
              <td><u>59.41</u></td>
              <td>29.63</td>
              <td><u>18.32</u></td>
            </tr>
          </tbody>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
        </table>
        
        <br>
        <br>
        <div class="content has-text-justified">
          <p>
            BaichuanSEED-SFT achieves the second best in code (MBPP and HumanEval), best in HellaSwag among all baselines, will underperforms in mathsmatics (MATH and GSM8K). To emphasis, we deliberately exclude downstream-task optimization to make BaichuanSEED "completely pure", such as upsamling on math and code, annealing trianing or introducing synthetic data. We discuss the potential of our model in the Discussion section of our paper.
          </p>
        </div>

        <h3 class="content">Downstream Tasks</h3>

        <table style="width:100%; border-collapse:collapse;">
          <thead>
            <tr>
              <th style="width: 26%;"></th>
              <th>Training Tokens</th>
              <th>MBPP (3-shot)</th>
              <th>HumanEval (0-shot)</th>
              <th>MATH (4-shot)</th>
              <th>GSM8K (4-shot)</th>
              <th>TriviaQA (0-shot)</th>
              <th>HellaSwag (0-shot)</th>
            </tr>
          </thead>
          <tr>
            <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
          </tr>
          <tbody>
            <tr>
              <td>Baichuan2-7B</td>
              <td>2.6T</td>
              <td>25.40</td>
              <td>17.68</td>
              <td>5.94</td>
              <td>25.02</td>
              <td>53.73</td>
              <td>67.56</td>
            </tr>
            <tr>
              <td>Baichuan2-13B</td>
              <td>2.6T</td>
              <td>30.88</td>
              <td>17.07</td>
              <td>10.68</td>
              <td>52.08</td>
              <td><u>58.73</u></td>
              <td>71.09</td>
            </tr>
            <tr>
              <td>Qwen1.5-7B</td>
              <td>3T</td>
              <td><u>36.60</u></td>
              <td><b>53.05</b></td>
              <td><b>21.08</b></td>
              <td><b>54.74</b></td>
              <td>50.92</td>
              <td><u>72.64</u></td>
            </tr>
            <tr>
              <td>Llama3-8B</td>
              <td>15T</td>
              <td><b>44.60</b></td>
              <td><u>26.22</u></td>
              <td>13.44</td>
              <td>50.11</td>
              <td><b>65.23</b></td>
              <td><b>74.54</b></td>
            </tr>
            <tr>
              <td>OLMo-7B</td>
              <td>2.5T</td>
              <td>21.00</td>
              <td>11.59</td>
              <td>1.72</td>
              <td>2.00</td>
              <td>49.81</td>
              <td>70.31</td>
            </tr>
            <tr>
              <td>MAP-Neo-7B</td>
              <td>4.5T</td>
              <td>25.90</td>
              <td>7.93</td>
              <td><u>15.14</u></td>
              <td><u>53.90</u></td>
              <td>54.80</td>
              <td>67.85</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td><b>BaichuanSEED</b></td>
              <td>3T</td>
              <td>34.12</td>
              <td>21.34</td>
              <td>9.84</td>
              <td>38.81</td>
              <td>45.92</td>
              <td>70.20</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td>Baichuan2-7B-Chat</td>
              <td>2.6T</td>
              <td>22.40</td>
              <td>15.24</td>
              <td>8.70</td>
              <td>32.37</td>
              <td>44.65</td>
              <td>69.18</td>
            </tr>
            <tr>
              <td>Baichuan2-13B-Chat</td>
              <td>2.6T</td>
              <td>26.30</td>
              <td>18.90</td>
              <td>8.62</td>
              <td>56.79</td>
              <td>53.47</td>
              <td>72.32</td>
            </tr>
            <tr>
              <td>Qwen1.5-7B-Chat</td>
              <td>3T</td>
              <td>12.58</td>
              <td><b>29.27</b></td>
              <td>13.12</td>
              <td>56.10</td>
              <td>10.22</td>
              <td>72.81</td>
            </tr>
            <tr>
              <td>Llama3-8B-Instruct</td>
              <td>15T</td>
              <td><b>52.17</b></td>
              <td>21.34</td>
              <td><u>25.62</u></td>
              <td><b>78.17</b></td>
              <td><b>63.37</b></td>
              <td>71.45</td>
            </tr>
            <tr>
              <td>OLMo-7B-SFT</td>
              <td>2.5T</td>
              <td>25.16</td>
              <td>19.51</td>
              <td>2.52</td>
              <td>17.66</td>
              <td>42.87</td>
              <td><u>72.62</u></td>
            </tr>
            <tr>
              <td>MAP-Neo-7B-SFT</td>
              <td>4.5T</td>
              <td>33.66</td>
              <td><b>29.27</b></td>
              <td><b>30.86</b></td>
              <td><u>70.28</u></td>
              <td><u>53.82</u></td>
              <td>68.48</td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
            <tr>
              <td><b>BaichuanSEED-SFT</b></td>
              <td>3T</td>
              <td><u>37.60</u></td>
              <td><u>23.17</u></td>
              <td>14.06</td>
              <td>53.98</td>
              <td>43.92</td>
              <td><b>73.03</b></td>
            </tr>
            <tr>
              <td colspan="8" style="text-align:center; border-top:1px solid black; border-bottom:1px solid black;"></td>
            </tr>
          </tbody>
        </table>
        
      </div>
    </div>
  </div>
</section>
<!-- Performance -->

<!-- BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>TBU</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
